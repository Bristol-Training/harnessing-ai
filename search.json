[
  {
    "objectID": "pages/900-summary.html",
    "href": "pages/900-summary.html",
    "title": "Summary",
    "section": "",
    "text": "Congratulations on completing this course on responsible LLM use. You now have the knowledge and tools to:\n\nUnderstand LLMs and their limitations\nProvide effective context while maintaining transparency\nEngineer prompts that enhance your capabilities\nNavigate ethical considerations responsibly\n\nSuccessful AI implementation requires ongoing commitment to learning, adaptation, and ethical reflection. The technology will continue to evolve, but our commitment to responsible use must remain constant.\n\nWhat are your next steps?\nImmediate (next 30 days):\n\nReview current AI guidance in your work\nIdentify opportunities for responsible AI application\n\nShort Term (next 3 months):\n\nImplement AI tools in low-risk applications\nDevelop personal templates and workflows\nShare experiences with colleagues\n\nLong Term (next 6-12 months):\n\nMentor others in responsible AI usage\nContribute to department policy development\nPursue advanced AI literacy training",
    "crumbs": [
      "Summary"
    ]
  },
  {
    "objectID": "pages/800-resources.html",
    "href": "pages/800-resources.html",
    "title": "Resources",
    "section": "",
    "text": "Guidelines\n\nUniversity of Bristol guidance\n\nIT Services Generative AI Guidance (UoB login required)\nUniversity Guidance on Generative AI in Education\nStudy Skills Resource on Using AI for Students\n\n\n\nOther guidance\n\nRussell Group Principles on generative AI in education\nUKRIO Embracing AI with integrity\n\n\n\n\nTraining resources\n\nPrompt Engineering\n\nOpenAI Prompt Engineering Guide\nAnthropic’s Guide to Claude\nGoogle’s AI Principles and Practices\n\n\n\nAI Ethics\n\nPartnership on AI\nAI Ethics Lab\nFuture of Humanity Institute\n\n\n\nOther\n\nOn the reg - podcast\nOne useful thing - blog",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "pages/700-examples.html",
    "href": "pages/700-examples.html",
    "title": "Examples",
    "section": "",
    "text": "A template to analyse and present workshop feedback\nThis is a generic template you can use to generate a report from feedback forms. You can try it using this synthetic dataset\n\n synthetic_workshop_feedback.xlsx.\n\n\n\n\n\n\n\nImportantData upload checklist\n\n\n\nBefore uploading any data file, verify that:\n\nYou have permission to use this content for your purposes\nAll personal identifiers have been removed (names, email addresses, phone numbers, etc.)\nNo sensitive or confidential information is included that users may have entered\nThe content complies with privacy regulations and institutional policies\n\n\n\n\nYou are a skilled analyst specialising in interpreting workshop participant feedback. You excel at identifying recurring themes, and spotting unusual, noteworthy, or conflicting viewpoints that emerge from responses.\nYour Analysis Will Include:\nStatistical Summary: Provide theme prevalence and confidence levels in your assessments.\nKey Observations: Identify outlier opinions, contradictory feedback, minority perspectives, and any concerning or unexpected patterns that stand apart from the consensus.\nComprehensive Overview: Deliver an overall assessment with workshop strengths, improvement areas, and actionable recommendations.\nReporting Format:\n\nExecutive summary with key insights\nQuantitative breakdown of responses\nMajor themes with supporting evidence\nNotable observations (unusual/conflicting feedback)\nSpecific recommendations for improvement\nUse graphics\nIn Word format\n\nUse clear, professional language with anonymised examples where helpful. Present findings in British English.\nThe feedback collected is enclosed\n\nMicrosoft Copilot (with GTP-5) example output\n\n Workshop_Feedback_Analysis_Report.docx",
    "crumbs": [
      "Examples"
    ]
  },
  {
    "objectID": "pages/600-ethics.html",
    "href": "pages/600-ethics.html",
    "title": "Ethics & Environmental Impact",
    "section": "",
    "text": "NoteKey Takeaways\n\n\n\n\nEnvironmental cost is significant - Use AI efficiently, batch queries, choose smaller models when appropriate\nAI has biases - Watch for gender, racial, socioeconomic, and geographic biases; request diverse examples explicitly\nMaintain human oversight - AI should enhance, not replace, your expertise and critical judgment\n\n\n\n\n\n\n\n\n\nImportantUniversity of Bristol AI guidance\n\n\n\nWe must be mindful of “environmental impacts, risks of bias and stereotyping, and ethical concerns about data privacy and security” when using AI tools.\nFind more →\n\n\n\nEnvironmental Impact\n\nThe Hidden Carbon Cost\nAI systems consume significant energy:\n\nTraining large models: It’s estimated that GPT-3’s training produced 85,000 kg of CO₂, emitting the same CO₂ as 112 cars running for a year.\nRunning AI services: Continuous energy use in data centers\nUser interactions: Each query requires computational resources\n\n\n\nSustainable AI Practices\nReduce Usage:\n\nBatch similar queries together\nUse AI for high-value tasks, not trivial ones\nCache and reuse outputs when possible\n\nChoose Efficiently:\n\nSelect providers committed to renewable energy\nConsider the computational cost of your requests\nAvoid unnecessary regeneration of content\n\nOffset Impact:\n\nSupport carbon offset initiatives\nAdvocate for renewable energy in AI infrastructure\n\n\n\n\nBias and Fairness in AI\n\nUnderstanding AI Bias\nAI systems inherit biases from their training data and can amplify existing societal inequalities.\nGender Bias\n\nAssociating certain professions with specific genders\nUsing gendered language inappropriately\nMaking assumptions about capabilities based on gender\n\nRacial and Ethnic Bias\n\nStereotypical associations with names or cultural references\nUnderrepresenting certain groups in examples\nMaking assumptions about backgrounds or capabilities\n\nSocioeconomic Bias\n\nAssuming access to resources or opportunities\nUsing examples that exclude certain economic backgrounds\nPrivileging certain educational or professional experiences\n\nGeographic Bias\n\nFocusing on Western/English-speaking perspectives\nMaking assumptions about local contexts\nOverlooking global south perspectives\n\n\n\n\n\n\n\nWarningBias Detection Questions\n\n\n\nWhen reviewing AI outputs, ask:\n\nRepresentation: Who is included and excluded in examples?\nLanguage: Are descriptions fair and respectful to all groups?\nAssumptions: What unstated assumptions are being made?\nPerspectives: Whose viewpoints are prioritized?\nStereotypes: Are any harmful generalizations present?\n\n\n\n\n\nMitigating Bias\nPrevention Strategies:\n\nRequest diverse examples explicitly\nAsk for multiple perspectives on controversial topics\nChallenge AI outputs that seem stereotypical\nInclude diverse voices in your verification process\n\nFor example, instead of:\n\nProvide examples of successful leaders,\n\ntry:\n\nProvide examples of successful leaders from diverse backgrounds, including different genders, ethnicities, and cultural contexts, explaining their varied leadership styles.",
    "crumbs": [
      "Ethics & Environmental Impact"
    ]
  },
  {
    "objectID": "pages/100-llm.html",
    "href": "pages/100-llm.html",
    "title": "What are LLMs?",
    "section": "",
    "text": "NoteKey Takeaways\n\n\n\n\nLLMs are mathematical models of language that maps similar words to nearby positions in a conceptual space\n\nThey predict which words are most likely to follow a given input, or prompt, based on patterns learned from vast amounts of text\nThey work by pattern matching, not true understanding, therefore they are not infallible sources of truth\n\nAlways verify important information from authoritative sources\nBe aware of bias in AI outputs and actively check for it\nUse critical thinking - if something sounds too precise or extreme, investigate further\nAlways follow University of Bristol AI guidance",
    "crumbs": [
      "What are LLMs?"
    ]
  },
  {
    "objectID": "pages/100-llm.html#section",
    "href": "pages/100-llm.html#section",
    "title": "What are LLMs?",
    "section": "",
    "text": "Exercise 1: Identifying LLM Limitations\nScenario: You’re preparing a briefing document about climate change impacts in Bristol.\nPrompt:\n\nWrite 2 sentences about how climate change is impacting Bristol\n\nReview the below outputs of three different LLM tools and identify potential issues:\n\nWhich output seems most reliable and why?\nWhat red flags can you identify in each response?\nHow would you verify the information before using it professionally?\n\n\nOutput A: “Bristol experienced its wettest year on record in 2023, with rainfall 40% above average. The Severn Estuary’s sea level has risen 15cm since 2000, directly threatening the city center.”\n\nOutput B: “Climate change will cause Bristol’s temperature to increase by exactly 3.2°C by 2050, making it uninhabitable for most residents. The city council has already approved plans to relocate the entire population to higher ground.”\n\nOutput C: “Bristol faces increased flooding risks due to climate change. The city has implemented various adaptation measures, though specific impacts vary by neighborhood and depend on global emission scenarios.”\n\n\n\n\n\n\nCautionSolution example\n\n\n\n\n\nOutput A: “Bristol experienced its wettest year on record in 2023, with rainfall 40% above average. The Severn Estuary’s sea level has risen 15cm since 2000, directly threatening the city center.”\nContains specific statistics that need verification. While plausible, the exact figures should be fact-checked against official sources.\n⚠️ Specific statistics require verification: “Wettest year on record” and “40% above average” are precise claims that need fact-checking\n⚠️ Sea level measurement: The specific “15cm since 2000” figure should be verified against official sources\n⚠️ Causal claim: Direct threat to city center needs geographic verification\n✅ Generally plausible: The types of impacts described are consistent with climate science\n\nOutput B: “Climate change will cause Bristol’s temperature to increase by exactly 3.2°C by 2050, making it uninhabitable for most residents. The city council has already approved plans to relocate the entire population to higher ground.”\nContains obvious hallucinations - overly precise predictions and extreme claims about population relocation that aren’t realistic.\n🚫 Unrealistic precision: “Exactly 3.2°C” - climate models deal in ranges, not exact predictions\n🚫 Extreme claims: “Uninhabitable for most residents” is hyperbolic and unsupported\n🚫 False information: No evidence of city council relocation plans\n🚫 Hallucination: Appears to invent specific policy decisions\n\nOutput C: “Bristol faces increased flooding risks due to climate change. The city has implemented various adaptation measures, though specific impacts vary by neighborhood and depend on global emission scenarios.”\nMost reliable approach - acknowledges uncertainty, speaks in general terms about known issues, doesn’t make unsupported specific claims.\n✅ Appropriately general: Makes broad, supportable statements\n✅ Acknowledges uncertainty: “Vary by neighborhood” and “depend on scenarios”\n✅ Realistic scope: Doesn’t make unsupported specific claims\n✅ Professional tone: Appropriate for briefing document\n\nKey Learning Points:\n\nBe suspicious of overly precise statistics without sources\nExtreme or alarmist language often indicates hallucination\nThe most useful AI outputs acknowledge uncertainty appropriately\nAlways verify specific factual claims before using professionally",
    "crumbs": [
      "What are LLMs?"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Welcome to this training course on the responsible use of Artificial Intelligence (AI) at the University.\nThe term Artificial Intelligence refers to computer systems that can perform tasks typically requiring human intelligence, such as understanding language, recognizing patterns, and making decisions. While the AI field has many years of research and development behind it, today’s increased popularity is largely driven by generative AI models—systems that can create new content like text, images, and code based on the patterns they’ve learned from vast amounts of data. You’ve probably heard of and used tools like ChatGPT and Copilot—here we’ll learn how to use them effectively and in compliance with University of Bristol guidance.\n\n\n\n\n\n\nImportantUniversity of Bristol AI guidance\n\n\n\nThis course is built on the foundation of the University’s commitment to responsible AI use. Our approach emphasizes:\n\nTransparency: Being open about when and how AI is used\nCritical Thinking: Using AI to support, not replace, intellectual rigor\nAcademic and Professional Integrity: Maintaining the highest standards of work\nEthical Considerations: Being mindful of bias, privacy, and environmental impact\n\nFind more →\n\n\n\nLearning Objectives\nBy the end of this course, you will be able to:\n\nUnderstand what LLMs are and their key capabilities and limitations\nApply responsible practices when using AI tools in your work\nCraft effective prompts that enhance rather than replace critical thinking\nNavigate ethical considerations including bias, privacy, and environmental impact\nImplement University of Bristol policies and guidelines in your AI usage\n\n\n\nAbout this course\nThese course materials have been designed and developed by Pau Erola, PhD (Jean Golding Institute, University of Bristol).\nThanks to Leo Gorman, Will Chapman, Rita Rasteiro, Richard Lane, James Thomas and Patricia Holley for their valuable comments and corrections.\nWe used the enhancement of AI in alignment with our institutional guidance:\n\nMicrosoft Copilot (GPT-4 & GPT-5)– Generation of synthetic datasets, documents and examples\nClaude (Sonnet 4) – Writing assistance, examples generation\nChatGPT (GPT-5) – Generation of figures\n\nFor queries related to this course please contact jgi-training@bristol.ac.uk.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "pages/310-prompting.html",
    "href": "pages/310-prompting.html",
    "title": "Prompting Frameworks",
    "section": "",
    "text": "NoteKey Takeaways\n\n\n\n\nHigh-quality prompts eliminate ambiguity - Clear context upfront gets better responses\nUse structured frameworks - CLEAR or BASE methods ensure complete prompts\nNever share sensitive data - No personal records, confidential research, or security information\nHigh-quality prompts that eliminate ambiguity are crucial for getting useful responses from Large Language Models. By providing comprehensive context upfront, you’re more likely to receive output that matches your requirements.\nScenario: You have been tasked with running a campaign to increase recruitment of European students for your University’s MSc Engineering programs.\nYou write the initial prompt:\nProblems with this prompt:\nWe can use some frameworks to standardize our queries and make sure they are complete and don’t forget any relevant details. There’s no right or wrong approach — different frameworks may work better for different AI tools and yourself, so use the one that works better for you.",
    "crumbs": [
      "Prompting Frameworks"
    ]
  },
  {
    "objectID": "pages/310-prompting.html#section",
    "href": "pages/310-prompting.html#section",
    "title": "Prompting Frameworks",
    "section": "",
    "text": "Exercise 3: Building Workplace Prompts\nScenario: You’re organizing a workshop on “Digital Skills for Academics” at the University of Bristol.\nTask: Create a prompt to help design the workshop agenda.\nRequirements:\n\nUse the CLEAR framework\nInclude appropriate constraints\nMaintain academic integrity\n\nReflection Questions:\n\nHow do your prompts ensure AI enhances rather than replaces your expertise?\nWhat verification steps would you take for each output?\nHow would you attribute AI assistance appropriately?\n\n\n\n\n\n\n\nCautionSolution example\n\n\n\n\n\nCreate a prompt to help design a workshop on “Digital Skills for Academics” at the University of Bristol.\n\nContext: I’m a University of Bristol academic organizing a “Digital Skills for Academics” workshop for faculty from various disciplines with mixed technical backgrounds.\nLength: Create a detailed 3-hour workshop agenda with 15-30 minute segments.\nExamples: Include interactive elements like hands-on activities and small group discussions, similar to effective pedagogical workshops I’ve attended.\nAction: Design an agenda that balances presentation, demonstration, and practice, progressing from basic to advanced concepts.\nRole: Act as an experienced workshop facilitator specializing in faculty development.\nAdditional Constraints:\n- Maximum 20 participants to allow for individual attention.\n- Must include time for networking and peer learning.\n- Consider accessibility needs and different learning styles.\n- Include evaluation/feedback collection methods.\n- Build in buffer time for technical issues.",
    "crumbs": [
      "Prompting Frameworks"
    ]
  },
  {
    "objectID": "pages/500-compliance.html",
    "href": "pages/500-compliance.html",
    "title": "Privacy & Compliance",
    "section": "",
    "text": "NoteKey Takeaways\n\n\n\n\nFollow University guidelines - Ensure academic integrity, maintain transparency, disclose AI use appropriately\nNever share sensitive data - No personal records, unpublished research, financial info, or security credentials\nAssess privacy risk first - Low risk (public info), medium risk (internal docs), high risk (personal/confidential)\nUse best practices - Anonymize data, use institutional accounts when available, check privacy policies\n\n\n\n\n\n\n\n\n\nImportantUniversity of Bristol AI guidance\n\n\n\nWe must be mindful of “environmental impacts, risks of bias and stereotyping, and ethical concerns about data privacy and security” when using AI tools.\nFind more →\n\n\n\nCompliance\nAdhering to University of Bristol guidelines is essential for maintaining the University’s legal compliance, protecting individuals’ privacy rights, and preserving our institutional reputation. Violations of these data protection principles can result in serious consequences.\nUniversity’s approach to AI is built on these foundational principles:\n\nEducational Excellence: AI should enhance, not replace, learning and critical thinking\nAcademic Integrity: Transparency and honesty in all AI usage\nEthical Responsibility: Consideration of bias, privacy, and societal impact\nEnvironmental Awareness: Sustainable and responsible technology use\nInclusive Innovation: Ensuring AI benefits all members of our community\n\n\n\n\n\n\n\n\nImportantUniversity of Bristol guidance\n\n\n\nNever share these with AI tools:\n🚫 Personal data: Student records, staff information, health data\n🚫 Confidential research: Unpublished findings, grant applications under review\n🚫 Commercial sensitive: Partnership agreements, financial information\n🚫 Legal privileged: Legal advice, disciplinary proceedings\n🚫 Security sensitive: Passwords, system configurations, access credentials\nFind more →\n\n\n\n\nData Privacy and Security\nWhen you use AI tools for work-related tasks, whether through web interfaces, APIs, or integrated applications, your data typically:\n\nTravels over the internet to AI company servers\nGets processed by AI systems you don’t control\n\nMay be stored temporarily or permanently by the AI provider\nCould potentially be used for training future AI models\nMight be subject to different legal jurisdictions\n\nUnderstanding this data flow is crucial for making informed decisions about what information you share with AI systems, especially when handling sensitive data or proprietary content. Always evaluate these risks and follow University guidance.”\n\nPrivacy Risk Assessment\nBefore using AI tools, ask:\nLow Risk ✅\n\nPublic information already available online\n\nGeneral knowledge questions\n\nAnonymous, aggregated data\n\nPublished research you’re summarizing\n\nMedium Risk ⚠️\n\nInternal documents with no personal data\n\nDraft policies before approval\n\nAcademic work in progress (with proper disclosure)\n\nHigh Risk ❌\n\nAny personal or confidential information\n\nUnpublished research data\n\nStudent or staff records\n\nCommercially sensitive material\n\n\n\nData Protection Best Practices\nImplementing proper data protection measures helps you minimizing privacy and security risks. Consider:\n\nUse institutional accounts when available (better data protection)\nAnonymize data before sharing with AI tools\nUse placeholder data for testing and training\nCheck privacy policies of AI tools you use\nFollow university guidelines on data classification\nConsider on-premises alternatives for sensitive work",
    "crumbs": [
      "Privacy & Compliance"
    ]
  },
  {
    "objectID": "pages/200-tools.html",
    "href": "pages/200-tools.html",
    "title": "LLMs tools",
    "section": "",
    "text": "Large Language Models (LLMs) have been rapidly introduced to workplace environments since ChatGPT’s breakthrough launch in late 2022. The landscape is quickly evolving, with major tech companies and AI startups releasing increasingly sophisticated models tailored for different use needs. When selecting an LLM consider factors such as data privacy policies, integration capabilities, cost structure, and specific feature sets that align with the University’s workflows. The following list covers the some of the most prominent options available today.\n\nMicrosoft Copilot copilot.microsoft.com\n\nFirst Release: March 2023\nStrengths: Deep integration with Microsoft Office suite\nBest for: Document creation, email assistance, Office workflows, enterprise productivity\nConsiderations: Enterprise versions offer better data protection; requires Microsoft 365 subscription\n\n\n\n\n\n\n\nImportantUniversity of Bristol AI guidance\n\n\n\nThe only recommended tool for the time being is Microsoft Copilot if you are looking to use generative AI to minimise data and privacy concerns.\nThe goal isn’t to avoid AI tools entirely, but to use them responsibly and effectively. Do not use AI apps, solutions, platforms that have not been formally approved by the University with personal data or any other confidential or sensitive information.\nFind more →\n\n\n\n\nChatGPT (OpenAI) chat.openai.com\n\nFirst Release: November 2022\nStrengths: Conversational, good for general tasks, coding assistance\nBest for: Writing assistance, brainstorming, general Q&A, code generation\nConsiderations: Data usage policies vary by version; enterprise versions available\n\n\n\nClaude (Anthropic) claude.ai\n\nFirst Release: March 2023\nStrengths: Helpful, harmless, honest approach; excellent for analysis\nBest for: Analysis, research assistance, ethical reasoning, document processing\nConsiderations: Strong focus on safety and accuracy; large context window\n\n\n\nGoogle Gemini gemini.google.com\n\nFirst Release: December 2023\nStrengths: Multimodal capabilities, integration with Google Workspace\nBest for: Research, document analysis, creative tasks, Google ecosystem integration\nConsiderations: Various models available (Nano, Pro, Ultra) with different capabilities\n\n\n\nGoogle NotebookLM notebooklm.google.com\n\nFirst Release: July 2023\nStrengths: Document-grounded AI, source-based research, podcast-style audio summaries\nBest for: Research analysis, document synthesis, creating audio overviews, academic work\nConsiderations: Works with your uploaded sources; Plus version offers 5x higher usage limits\n\n\n\nMeta Llama llama.meta.com\n\nFirst Release: February 2023\nStrengths: Open-source, customizable, strong performance\nBest for: Custom applications, on-premise deployment, research\nConsiderations: Requires technical expertise; various sizes available (7B to 405B parameters)\n\n\n\nPerplexity AI perplexity.ai\n\nFirst Release: August 2022\nStrengths: Real-time web search integration, source citations\nBest for: Research, fact-checking, current information retrieval\nConsiderations: Excellent for staying current with latest information\n\n\n\n\n\nProper Attribution\nWhen artificial intelligence tools significantly contribute to your work, proper attribution is essential for maintaining transparency and ethical standards. Failing to acknowledge AI assistance where it played a substantial role can mislead audiences about the true nature of your work’s creation.\nExample Attribution: “Initial drafts of this literature review were developed with assistance from Claude (Sonnet 4). All sources were independently verified, analysis was conducted by the author, and conclusions represent the author’s professional judgment.”\nDo you require more thorough record-keeping? Use a documentation form instead\n\n LLM_Usage_Documentation_Form.docx",
    "crumbs": [
      "LLMs tools"
    ]
  },
  {
    "objectID": "pages/300-context.html",
    "href": "pages/300-context.html",
    "title": "Context matters",
    "section": "",
    "text": "NoteKey Takeaways\n\n\n\n\nContext is all the information you give LLMs to understand your request (role, purpose, audience, requirements)\nBe specific, not vague - Clear context gets relevant responses; vague prompts get generic ones\nRefine iteratively - Start basic, review output, add details, repeat until satisfied\nNever share sensitive data - No personal records, unpublished research, or confidential information\nLarge Language Models work by navigating through vast embedding spaces—multidimensional representations of knowledge and concepts. Vague or poorly defined context can lead the model to explore irrelevant areas of this space, producing generic or off-target responses. Well-crafted context act as precise navigation instructions, guiding the model to the most relevant knowledge areas and ensuring outputs that match your specific needs and context.",
    "crumbs": [
      "Context matters"
    ]
  },
  {
    "objectID": "pages/300-context.html#section",
    "href": "pages/300-context.html#section",
    "title": "Context matters",
    "section": "",
    "text": "Exercise 2: Context Writing\nTransform a vague prompt into an effective, contextualised request. Use an AI tool of your choice to observe the difference that context have on your request.\nScenario: You need to create a policy document about flexible working arrangements for your department.\nInitial prompt:\n\nWrite a 1-page-long flexible working policy\n\n\n\n\n\n\n\nCautionSolution example\n\n\n\n\n\n\nInitial prompt\n\nWrite a 1-page-long flexible working policy\n\nSee Claude (Sonnet 4) answer here\n\n flexible-working-policy.pdf\n\n\n\nContextualised prompt\n\nContext. I am a Departmental Operations Manager in the University of Bristol Engineering Department, developing a flexible working policy for our 45 academic staff and 12 professional services staff.\nPurpose. Create a clear policy that supports work-life balance while maintaining research productivity, teaching quality, and team collaboration, complying with university HR policies and employment law.\nAudience. Department staff (mix of early-career and senior academics, technical and administrative professionals) and department leadership who need practical guidance.\nScope. Cover remote work, flexible hours, compressed schedules, and term-time arrangements. Exclude sabbatical leave (covered elsewhere) and temporary adjustments (covered by separate policy).\nFormat. 1 page professional document with clear sections, bullet points for key procedures, and practical examples.\n\nSee Claude (Sonnet 4) answer here\n\n engineering-department-flexible-working-policy.pdf",
    "crumbs": [
      "Context matters"
    ]
  },
  {
    "objectID": "pages/400-conversational.html",
    "href": "pages/400-conversational.html",
    "title": "Conversational Learning",
    "section": "",
    "text": "NoteKey Takeaways\n\n\n\n\nUse AI as learning partner, not task executor - Ask “why” and “how” to understand processes, not just get outputs\nShift from efficiency to learning mindset - Move from “give me the answer” to “walk me through the thinking”\nChallenge AI reasoning - Test explanations, ask about limitations, play devil’s advocate\nMost people approach AI with a transactional mindset, asking for document summaries, presentation outlines, email drafts, or quick recommendations. While this approach produces immediate outputs, it creates a fundamental problem: you receive results without gaining insight into the underlying processes that created them.\nThe learning partnership approach transforms this dynamic entirely. Instead of simply requesting outputs, you engage AI as a collaborative thinking partner. You might ask an AI to explain why it structured a summary in a particular way, what principles guided its outline choices, or how it determined the appropriate tone for professional communication. This shift in questioning style ensures that every interaction becomes an opportunity to understand processes and develop transferable skills.\nThe difference in outcomes is profound. Traditional approaches leave you dependent on AI for similar future tasks, while learning partnerships build your capacity to handle increasingly complex challenges independently.",
    "crumbs": [
      "Conversational Learning"
    ]
  },
  {
    "objectID": "pages/400-conversational.html#section",
    "href": "pages/400-conversational.html#section",
    "title": "Conversational Learning",
    "section": "",
    "text": "Exercise 4: Conversational Learning Practice\nScenario: You need to improve staff participation in development courses.\n1. Traditional Approach. First, ask AI directly: “Give me strategies to increase staff participation in development courses.”\n2. Learning Partnership Approach. Now, have a learning conversation. Use these conversation starters:\n\nExplore the problem: “Help me understand why staff might be reluctant to participate. What factors typically influence participation?”\nQuestion assumptions: “I’m assuming low participation is a problem - help me examine this assumption. When might quiet classrooms actually be effective?”\nBuild understanding: “What principles of group dynamics and learning psychology should guide my approach?”\n\n3. Reflection. Compare the two approaches:\n\nWhich gave you more insight into the underlying issues?\nWhich better prepared you to handle similar challenges in the future?\n\n\n\n\n\n\n\nCautionSolution example\n\n\n\n\n\nBased on Copilot (GPT-4 architecture).\n\nIn 200 words, give me strategies to increase staff participation in development courses.\n\n\n\n\n\n\n\nTo increase staff participation in development courses, focus on relevance, accessibility, and motivation. Begin by aligning courses with individual career goals and team objectives. When staff see how training supports their growth or contributes to project success, they’re more likely to engage.\nMake learning convenient by offering flexible formats—online modules, short sessions, or on-demand access. Ensure courses are scheduled during work hours and don’t conflict with key responsibilities.\nLeadership plays a key role: managers should actively promote courses, participate themselves, and recognize staff who engage. This sets a positive example and reinforces a culture of learning.\nIncentivize participation with certificates, digital badges, or public recognition. Link course completion to tangible benefits like eligibility for new roles or responsibilities.\nRegularly communicate the value of development opportunities through internal channels. Use success stories or testimonials to show real-world impact.\nFinally, gather feedback to improve course offerings and involve staff in shaping future content. When employees feel heard and see their input reflected, they’re more likely to participate.\n\n\n\n\nIn 200 words, help me understand why staff might be reluctant to participate. What factors typically influence participation?\n\n\n\n\n\n\n\nStaff may be reluctant to participate in development courses due to a mix of practical constraints and psychological factors. One of the most common barriers is lack of time—employees often feel overwhelmed by daily responsibilities and may not see how training fits into their workload. If courses are scheduled during busy periods or require significant time commitments, participation drops.\nAnother key factor is perceived relevance. Staff are less likely to engage if they don’t see how the course content applies to their role or career goals. Generic or outdated offerings can feel disconnected from real work challenges.\nManagerial support also plays a crucial role. If leaders don’t actively encourage participation or fail to prioritize learning, staff may assume it’s not valued. Conversely, when managers model engagement and link training to performance reviews or promotions, uptake improves.\nFear of failure or judgment can also deter participation, especially in group settings or courses with assessments. Some staff may worry about exposing gaps in their knowledge.\nFinally, accessibility issues—such as limited formats, lack of remote options, or unclear instructions—can make courses hard to attend.\n\n\n\n\nI’m assuming low participation is a problem - help me examine this assumption. When might quiet classrooms actually be effective? Don’t use more than 200 words.\n\n\n\n\n\n\n\nLow participation in development courses is often seen as a problem, but it’s worth examining whether that assumption always holds. A quiet classroom doesn’t necessarily mean disengagement—it can reflect deep concentration, a preference for self-paced learning, or a mismatch between delivery style and learning preferences.\nSome staff may prefer asynchronous formats, such as recorded sessions or written materials, which don’t require visible participation. Others may be absorbing content passively before applying it practically. In highly skilled or research-focused environments, staff might already be learning informally through projects, peer collaboration, or independent study.\nAdditionally, low attendance might indicate that the course isn’t needed, rather than a lack of interest. If staff already possess the skills being taught, or if the course doesn’t align with current priorities, opting out can be a rational choice.\nRather than focusing solely on numbers, it’s more useful to assess impact—are those who attend applying what they learn? Are staff developing in other ways? Quiet classrooms can still be effective if they support meaningful, relevant learning.\n\n\n\n\nWhat principles of group dynamics and learning psychology should guide my approach? Don’t use more than 200 words.\n\n\n\n\n\n\n\nYour approach to increasing participation in development courses should be guided by key principles from group dynamics and learning psychology:\n\nSocial Influence & Norms. People are more likely to engage when they see peers doing the same. Creating visible norms around participation—such as team-wide involvement or manager endorsement—can foster a sense of collective commitment.\nPsychological Safety. Staff need to feel safe to ask questions, make mistakes, and share ideas. A supportive learning environment encourages participation, especially in group settings.\nSelf-Determination Theory. Motivation increases when learners feel autonomy, competence, and relatedness. Offer choices in course formats, ensure content builds confidence, and foster connection through collaborative activities.\nConstructivist Learning. People learn best when they actively construct knowledge through experience. Encourage reflection, discussion, and application to real tasks rather than passive consumption.\nGroup Cohesion. Strong group bonds improve engagement. Use team-based learning, peer mentoring, or shared goals to build trust and accountability.",
    "crumbs": [
      "Conversational Learning"
    ]
  }
]